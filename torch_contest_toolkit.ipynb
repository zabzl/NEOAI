{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c7173e",
   "metadata": {},
   "source": [
    "# PyTorch Contest Toolkit\n",
    "Полный набор шаблонов и утилит для решения задач **NLP, CV и табличного ML** на олимпиадных конкурсах.\n",
    "*Версия:* 2025-05-06\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be0fbd",
   "metadata": {},
   "source": [
    "## Содержание\n",
    "1. [Установка окружения](#setup)\n",
    "2. [Общие утилиты](#utils)\n",
    "3. [Базовый тренировочный движок](#engine)\n",
    "4. [Computer Vision](#cv)\n",
    "5. [Natural Language Processing](#nlp)\n",
    "6. [Tabular / General ML](#ml)\n",
    "7. [Сохранение и загрузка моделей](#ckpt)\n",
    "8. [Дополнительные приёмы](#extras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3869e89",
   "metadata": {},
   "source": [
    "## 1. Установка окружения <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если работаете в окружении без PyTorch:\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# Другие полезные пакеты (по необходимости):\n",
    "# !pip install transformers sentencepiece scikit-learn numpy pandas matplotlib tqdm albumentations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77312fa",
   "metadata": {},
   "source": [
    "## 2. Общие утилиты <a id='utils'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, math, time, json, copy, gc, logging, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "set_seed()\n",
    "device = get_device()\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b08a5",
   "metadata": {},
   "source": [
    "## 3. Базовый тренировочный движок <a id='engine'></a>\n",
    "Единый цикл обучения и валидации, применимый к любому типу задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80507fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, scheduler=None,\n",
    "                 mixed_precision=True, grad_clip=None):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=mixed_precision)\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "    def _step(self, batch, train=True):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=self.scaler.is_enabled()):\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "        if train:\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if self.grad_clip:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        return loss.item(), outputs.detach().cpu(), targets.detach().cpu()\n",
    "\n",
    "    def loop(self, loader, train=True):\n",
    "        self.model.train() if train else self.model.eval()\n",
    "        epoch_loss, preds, gts = 0.0, [], []\n",
    "        with torch.set_grad_enabled(train):\n",
    "            for batch in tqdm(loader, leave=False):\n",
    "                loss, out, tgt = self._step(batch, train)\n",
    "                epoch_loss += loss * len(tgt)\n",
    "                preds.append(out)\n",
    "                gts.append(tgt)\n",
    "        preds = torch.cat(preds)\n",
    "        gts = torch.cat(gts)\n",
    "        return epoch_loss / len(loader.dataset), preds, gts\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None, epochs=10,\n",
    "            metric_fn=None, ckpt_path=None, early_stop=None):\n",
    "        best_metric, patience = None, 0\n",
    "        for epoch in range(1, epochs+1):\n",
    "            tr_loss, *_ = self.loop(train_loader, train=True)\n",
    "            if self.scheduler: self.scheduler.step()\n",
    "            if val_loader:\n",
    "                val_loss, preds, gts = self.loop(val_loader, train=False)\n",
    "                metric_val = metric_fn(preds, gts) if metric_fn else val_loss\n",
    "                print(f'Epoch {epoch} | train: {tr_loss:.4f} | val: {val_loss:.4f} | metric: {metric_val:.4f}')\n",
    "                if best_metric is None or metric_val > best_metric:\n",
    "                    best_metric, patience = metric_val, 0\n",
    "                    if ckpt_path:\n",
    "                        torch.save(self.model.state_dict(), ckpt_path)\n",
    "                        print('  ✔ Новый лучший чекпоинт сохранён')\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    if early_stop and patience >= early_stop:\n",
    "                        print('Early stopping!')\n",
    "                        break\n",
    "            else:\n",
    "                print(f'Epoch {epoch} | loss: {tr_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d99d8",
   "metadata": {},
   "source": [
    "## 4. Computer Vision <a id='cv'></a>\n",
    "Примеры датасета, аугментаций, модели и запуска обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764299fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# ---- Dataset & transforms ----\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Example using ImageFolder (понадобится структура данных class_name/xxx.jpg)\n",
    "# train_ds = datasets.ImageFolder('train_images', transform=train_tfms)\n",
    "# val_ds   = datasets.ImageFolder('val_images', transform=val_tfms)\n",
    "\n",
    "# ---- Model ----\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64*56*56, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# resnet = models.resnet18(weights=None)  # разрешено — без предобученных весов\n",
    "# resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "def accuracy(preds, gts):\n",
    "    return (preds.argmax(dim=1) == gts).float().mean().item()\n",
    "\n",
    "# ---- Training example (commented) ----\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "# model = SmallCNN(num_classes=len(train_ds.classes))\n",
    "# trainer = Trainer(model, nn.CrossEntropyLoss(),\n",
    "#                   torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "#                   scheduler=None, mixed_precision=True, grad_clip=1.0)\n",
    "# trainer.fit(train_loader, val_loader, epochs=10, metric_fn=accuracy,\n",
    "#             ckpt_path='best_cnn.pth', early_stop=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a29357",
   "metadata": {},
   "source": [
    "## 5. Natural Language Processing <a id='nlp'></a>\n",
    "Шаблон для классификации текста с Embedding + LSTM.\n",
    "Для токенизации можно использовать `torchtext`, `sentencepiece`, или собственный словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ---- Vocabulary ----\n",
    "class Vocab:\n",
    "    def __init__(self, tokens, min_freq=2):\n",
    "        from collections import Counter\n",
    "        counter = Counter(tokens)\n",
    "        self.itos = ['<pad>', '<unk>'] + [t for t,c in counter.items() if c >= min_freq]\n",
    "        self.stoi = {t:i for i,t in enumerate(self.itos)}\n",
    "    def encode(self, tokens):\n",
    "        return [self.stoi.get(t, 1) for t in tokens]\n",
    "\n",
    "# ---- Dataset ----\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None):\n",
    "        self.raw_texts = [t.split() for t in texts]\n",
    "        self.labels = torch.tensor(labels)\n",
    "        if vocab is None:\n",
    "            vocab = Vocab([tok for txt in self.raw_texts for tok in txt])\n",
    "        self.vocab = vocab\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.vocab.encode(self.raw_texts[idx])), self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return seqs_padded, torch.tensor(labels)\n",
    "\n",
    "# ---- Model ----\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "# ---- Training example (commented) ----\n",
    "# vocab = Vocab([tok for txt in train_texts for tok in txt.split()])\n",
    "# train_ds = TextDataset(train_texts, train_labels, vocab)\n",
    "# val_ds   = TextDataset(val_texts, val_labels, vocab)\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "# model = LSTMClassifier(len(vocab.itos), 128, 256, num_classes=2)\n",
    "# trainer = Trainer(model, nn.CrossEntropyLoss(),\n",
    "#                   torch.optim.Adam(model.parameters(), lr=2e-3),\n",
    "#                   mixed_precision=False)\n",
    "# trainer.fit(train_loader, val_loader, epochs=6, metric_fn=accuracy, ckpt_path='best_lstm.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225135a6",
   "metadata": {},
   "source": [
    "## 6. Табличный / общий ML <a id='ml'></a>\n",
    "MLP для регрессии/классификации с числовыми признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_layers=[256,128], out_dim=1, drop_p=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden_layers:\n",
    "            layers += [nn.Linear(last, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(drop_p)]\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# Dataset skeleton\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "# Metric examples\n",
    "def rmse(preds, gts): return ((preds.squeeze() - gts.float())**2).mean().sqrt().item()\n",
    "def acc_cls(preds, gts): return (preds.argmax(1) == gts).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f3efb",
   "metadata": {},
   "source": [
    "## 7. Сохранение / загрузка моделей <a id='ckpt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(model, path, extras=None):\n",
    "    torch.save({'state_dict': model.state_dict(),\n",
    "                'extras': extras or {}}, path)\n",
    "\n",
    "def load_ckpt(model, path, map_location=None):\n",
    "    chkpt = torch.load(path, map_location=map_location or device)\n",
    "    model.load_state_dict(chkpt['state_dict'])\n",
    "    return chkpt.get('extras', {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bd70c",
   "metadata": {},
   "source": [
    "## 8. Дополнительные приёмы <a id='extras'></a>\n",
    "- **Grad Accumulation**: аккумулируйте градиенты для больших батчей.\n",
    "- **CosineAnnealingLR / OneCycleLR**: гибкое расписание обучения.\n",
    "- **K-Fold Cross-Validation**: усреднение моделей.\n",
    "- **Snapshot Ensembling**: сохраняйте несколько чекпоинтов.\n",
    "- **Mixed Precision (`torch.cuda.amp`)**: ускоряет и экономит память на GPU.\n",
    "- **Profiling**: `torch.profiler`, `nvprof`, `nsys`.\n",
    "- **Debugging**: `detect_anomaly`, `torch.autograd.set_detect_anomaly(True)`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
